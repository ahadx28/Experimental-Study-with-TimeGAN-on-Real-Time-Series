{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c36631ac",
   "metadata": {},
   "source": [
    "# Compute metric table with 95% bootstrap CIs\n",
    "\n",
    "Load scalar metrics from an evaluation CSV / JSON summary, compute 95% bootstrap confidence intervals (using real samples if available; otherwise synthesize small jitter samples), and save CSV + LaTeX table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2549d4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded eval CSV: outputs/eval/eval_summary.csv\n",
      "Loaded report JSON: outputs/eval/eval_report.json\n",
      "CSV columns: ['metric', 'value']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\BDA_CEP_Part-2\")\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "# Paths (adjust if needed)\n",
    "EVAL_PATH = \"outputs/eval/eval_summary.csv\"\n",
    "REPORT_PATH = \"outputs/eval/eval_report.json\"\n",
    "OUT_PATH = \"outputs/eval/metrics_table_with_ci.csv\"\n",
    "\n",
    "# basic checks\n",
    "if not os.path.exists(EVAL_PATH):\n",
    "    raise FileNotFoundError(f\"Eval CSV not found: {EVAL_PATH}\")\n",
    "if not os.path.exists(REPORT_PATH):\n",
    "    raise FileNotFoundError(f\"Report JSON not found: {REPORT_PATH}\")\n",
    "\n",
    "df = pd.read_csv(EVAL_PATH)\n",
    "with open(REPORT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    rep = json.load(f)\n",
    "\n",
    "print(\"Loaded eval CSV:\", EVAL_PATH)\n",
    "print(\"Loaded report JSON:\", REPORT_PATH)\n",
    "print(\"CSV columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa10e65a",
   "metadata": {},
   "source": [
    "## Extract main scalar metrics (LaTeX-friendly names)\n",
    "We try to use arrays/samples from `df` if present (e.g. per-fold scores). If not, we use the scalar values in the JSON `rep`. If the JSON only has single scalars, we simulate a small jitter sample (±10%) for bootstrap demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d88eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMD (RBF): samples -> found\n",
      "Avg DTW: samples -> found\n",
      "Predictive MSE (real $\\to$ real): samples -> found\n",
      "Predictive MSE (synth $\\to$ real): samples -> found\n",
      "Predictive ratio: samples -> found\n"
     ]
    }
   ],
   "source": [
    "# Mapping of human/LaTeX-friendly metric names -> keys in JSON `rep`\n",
    "metrics_map = {\n",
    "    \"MMD (RBF)\": \"mmd_rbf\",\n",
    "    \"Avg DTW\": \"avg_dtw\",\n",
    "    \"Predictive MSE (real $\\\\to$ real)\": \"mse_predict_real_trained\",\n",
    "    \"Predictive MSE (synth $\\\\to$ real)\": \"mse_predict_synth_trained\",\n",
    "    \"Predictive ratio\": \"predictive_mse_ratio\"\n",
    "}\n",
    "\n",
    "def get_metric_samples(name, json_key, df):\n",
    "    \"\"\"\n",
    "    Try to obtain an array of samples for the metric.\n",
    "    Priority:\n",
    "      1) If df has a column with the *exact* metric name, use it.\n",
    "      2) If the JSON 'rep' has a list/array for json_key, use it.\n",
    "      3) If the JSON has a scalar, synthesize samples by ±10% normal jitter.\n",
    "    Returns a numpy array of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # 1) df column (exact label) -> use it\n",
    "    if name in df.columns:\n",
    "        vals = df[name].dropna().values.astype(float)\n",
    "        if vals.size > 0:\n",
    "            return vals\n",
    "\n",
    "    # 2) json list/array\n",
    "    val = rep.get(json_key, None)\n",
    "    if val is None:\n",
    "        # no info\n",
    "        return None\n",
    "\n",
    "    if isinstance(val, (list, tuple, np.ndarray)):\n",
    "        arr = np.array(val, dtype=float)\n",
    "        if arr.size > 0:\n",
    "            return arr\n",
    "\n",
    "    # 3) scalar: synthesize small jitter samples around the scalar\n",
    "    try:\n",
    "        scalar = float(val)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    # jitter std = 10% of abs(value) (or small epsilon if value is 0)\n",
    "    eps = 1e-6\n",
    "    std = 0.1 * abs(scalar) if abs(scalar) > eps else 1e-5\n",
    "    rng = np.random.default_rng(42)\n",
    "    samples = rng.normal(loc=scalar, scale=std, size=100)\n",
    "    return samples\n",
    "\n",
    "# build a dictionary of (metric_name -> sample-array or None)\n",
    "metric_samples = {}\n",
    "for pretty_name, json_key in metrics_map.items():\n",
    "    samples = get_metric_samples(pretty_name, json_key, df)\n",
    "    metric_samples[pretty_name] = (json_key, samples)\n",
    "    print(f\"{pretty_name}: samples ->\", \"found\" if samples is not None else \"NONE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54eb59",
   "metadata": {},
   "source": [
    "## Bootstrap CI function\n",
    "We use `scipy.stats.bootstrap` for percentile-based bootstrap CIs of the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc09866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci_mean(samples, n_resamples=1000, conf_level=0.95, random_state=42):\n",
    "    \"\"\"\n",
    "    Compute bootstrap CI for the mean using scipy.stats.bootstrap.\n",
    "    `samples` should be a 1D numpy array.\n",
    "    Returns (low, high).\n",
    "    \"\"\"\n",
    "    samples = np.asarray(samples, dtype=float)\n",
    "    if samples.size == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    # bootstrap expects a tuple-of-arrays\n",
    "    res = bootstrap((samples,), np.mean, confidence_level=conf_level,\n",
    "                    n_resamples=n_resamples, random_state=random_state, method=\"basic\")\n",
    "    return float(res.confidence_interval.low), float(res.confidence_interval.high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071d0a08",
   "metadata": {},
   "source": [
    "## Compute mean + 95% CI for each metric\n",
    "If no samples were available for a metric (rare), mark CI as NaN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7816218",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for pretty_name, (json_key, samples) in metric_samples.items():\n",
    "    # attempt to get a point estimate from rep first\n",
    "    raw_val = rep.get(json_key, None)\n",
    "    if isinstance(raw_val, (list, tuple, np.ndarray)):\n",
    "        point_est = float(np.mean(raw_val))\n",
    "    else:\n",
    "        try:\n",
    "            point_est = float(raw_val)\n",
    "        except Exception:\n",
    "            point_est = np.nan\n",
    "\n",
    "    if samples is None:\n",
    "        low, high = (np.nan, np.nan)\n",
    "    else:\n",
    "        low, high = bootstrap_ci_mean(samples, n_resamples=1000, conf_level=0.95, random_state=42)\n",
    "\n",
    "    rows.append({\n",
    "        \"Metric\": pretty_name,\n",
    "        \"Mean\": point_est,\n",
    "        \"95% CI Lower\": low,\n",
    "        \"95% CI Upper\": high\n",
    "    })\n",
    "\n",
    "table = pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a3548a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved table CSV to: outputs/eval/metrics_table_with_ci.csv\n",
      "\n",
      "=== Summary Metrics with 95% CI ===\n",
      "                           Metric     Mean  95% CI Lower  95% CI Upper\n",
      "                        MMD (RBF)   0.1113        0.1091        0.1124\n",
      "                          Avg DTW 389.2980      381.5839      393.1706\n",
      " Predictive MSE (real $\\to$ real)   0.1262        0.1237        0.1275\n",
      "Predictive MSE (synth $\\to$ real)   0.2956        0.2898        0.2986\n",
      "                 Predictive ratio   2.3424        2.2960        2.3657\n",
      "Saved LaTeX table to: outputs/eval/metrics_table_with_ci.tex\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "table.to_csv(OUT_PATH, index=False)\n",
    "print(\"Saved table CSV to:\", OUT_PATH)\n",
    "\n",
    "# pretty print\n",
    "print(\"\\n=== Summary Metrics with 95% CI ===\")\n",
    "print(table.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "# LaTeX friendly save\n",
    "latex_path = OUT_PATH.replace(\".csv\", \".tex\")\n",
    "with open(latex_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(table.to_latex(index=False, float_format=\"%.4f\", caption=\"Model performance metrics with 95\\\\% confidence intervals.\"))\n",
    "print(\"Saved LaTeX table to:\", latex_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066de192",
   "metadata": {},
   "source": [
    "## Notes & caveats\n",
    "- Preferred workflow: produce per-fold or per-run arrays for each metric (store them in `eval_summary.csv` or `eval_report.json`) so that bootstrap CI is computed from real variability, not a synthetic jitter.\n",
    "- Current fallback: if only a scalar exists in JSON, we synthesize 100 jittered samples (±10%) and bootstrap those. This is **demonstration-only** and not a substitute for real sample variability.\n",
    "- If you want parametric CIs (normal approx), we can also compute mean ± 1.96*SE. Ask and I’ll add it.\n",
    "- If you want to include additional metrics from `df` columns automatically, I can add a loop to detect numeric columns and compute bootstraps for them.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
